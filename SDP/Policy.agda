open import SDP.SDP
open import Monad

module SDP.Policy {M} {isMonad : Monad M} (sdp : SDP isMonad) where

open SDP sdp

open import Data.Nat.Base
open import Agda.Builtin.Equality

private variable
  t n : ‚Ñï

-- Policies

Policy : ‚Ñï ‚Üí Set
Policy t = (x : X t) ‚Üí Y x

-- Policy sequences

data PolicySeq (t : ‚Ñï) : (n : ‚Ñï) ‚Üí Set where
  [] : PolicySeq t 0
  _‚à∑_ : (p : Policy t) ‚Üí (ps : PolicySeq (suc t) n) ‚Üí PolicySeq t (suc n)

infixr 5 _‚à∑_

-- Computing the value of a policy sequence

val : PolicySeq t n ‚Üí X t ‚Üí Val
val [] x = ùüò
val (p ‚à∑ ps) x =
  let y = p x
      mx‚Ä≤ = next x y
  in  measure (fmap (reward x y ‚äï‚Çó val ps) mx‚Ä≤)

-- Note that Bellman's equality holds definitionally with this definition of val

BellmanEq : (p : Policy t) (ps : PolicySeq (suc t) n) (x : X t)
          ‚Üí val (p ‚à∑ ps) x ‚â° measure (fmap (reward x (p x) ‚äï‚Çó val ps) (next x (p x)))
BellmanEq p ps x = refl

-- Optimal policy sequences

OptPolicySeq : PolicySeq t n ‚Üí Set
OptPolicySeq {t} {n} ps = (ps‚Ä≤ : PolicySeq t n) ‚Üí val ps‚Ä≤ ‚â§‚Çó val ps

-- Optimal extensions

OptExt : PolicySeq (suc t) n ‚Üí Policy t ‚Üí Set
OptExt ps p = ‚àÄ p‚Ä≤ ‚Üí val (p‚Ä≤ ‚à∑ ps) ‚â§‚Çó val (p ‚à∑ ps)
